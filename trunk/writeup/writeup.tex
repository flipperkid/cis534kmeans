\documentclass[10pt,journal]{IEEEtran}
\usepackage{cite, graphicx, subfigure, amsmath} 
\usepackage{algorithm, algorithmic}
\usepackage{graphicx}
\interdisplaylinepenalty=2500

% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.


\begin{document}
% paper title
\title{Parallelizing k-means}


% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
\author{Chris Jordan,
Victor Janmey,
and Raymond W. Ko}% <-this % stops a space


% The paper headers
%\markboth{<+Journal of VIM-\LaTeX\ Class Files,~Vol.~1, No.~8,~August~2002+>}{
%<+Bhardwaj \MakeLowercase{\textit{et al.}+>}: <+Skeleton of IEEEtran.cls for Journals in VIM-Latex+>}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.


% If you want to put a publisher's ID mark on the page
% (can leave text blank if you just want to see how the
% text height on the first page will be reduced by IEEE)
%\pubid{0000--0000/00\$00.00~\copyright~2002 IEEE}


% use only for invited papers
%\specialpapernotice{(Invited Paper)}


% make the title area
\maketitle

\begin{abstract}
Abstract goes where? Abstract goes here!
\end{abstract}

\begin{keywords}
k-means, parallel programming
\end{keywords}

\section{Introduction}
\PARstart{T}{he} k-means algorithm groups particles into a fixed number of clusters.
It has an extraordinary amount of uses including in fields such as
machine learning, computer vision, and network analysis.

In the case of machine learning, it is useful in unsupervised learning methods, 
dividing data for more specific classifiers, and the selection of of examples for k-nearest neighbors.

For computer vision, one frequest task is to segment images using the difference in pixel intensity as a distance
metric. One can then use the k-means algorithm to do the above and estimate object boundaries, or use it as a cue for other algorithms
that perform object recognition.

Additinally, the use of k-means is not limited to just images, in network analysis it can be used to find communities.

However, in this paper, we will focus on the problem of image segmentation, in which which we will try to tune the code
to run as fast as possible.

\section{Analysis of Algorithm}
Before focusing on the acutal implementation, the k-means algorithm
was examined to see what sections could be effectively parallelized.

\subsection{\textit{k-means}}
The k-means algorithm basically consists of three steps:
choosing $k$ intial clusters, an assignment step, and an
update step. In the assignement step, particles get assigned
to clusters. In the update step, the new mean of a cluster
is recomputed by the making the average of all the values
of the particles in a cluster.

\begin{algorithm}
\caption{k-means++ algorithm}
\begin{algorithmic}[1]
\STATE{choose $k$ initial means as centers}
\FOR{each particle $p$}
    \FOR{each mean $k$}
        \STATE{calculate distance from $p$ to $k$}
        \IF {distance $<$ previous calculated distance}
            \STATE{mark down mean}
        \ENDIF
    \ENDFOR
    \STATE{put $k$ into last marked down cluster with mean}
\ENDFOR
\FOR{each cluster $c$}
    \STATE{sum $\leftarrow$ 0}
    \FOR{each partice $p$ in $c$}
        \STATE{sum $\leftarrow$ sum $+$ p}
    \ENDFOR
    \STATE{new mean of cluster $c$ $\leftarrow$ sum / $|c|$}
\ENDFOR
\STATE{repeat steps 2 to 17 until new means converge and no longer change}
\end{algorithmic}
\end{algorithm}

An initial and precursory glance at the algorithm indicates that there are parts that are
easy to parallelize.

The initial steps for choosing $k$ can be done in any fashion, and since they are not
dependent on each other, can simply be randomly chosen in parallel.

The second step, the assignment step, is the time dominating factor in the calculation since
it is $O(kn)$ due to the fact that for every particle, it must consider every $k$-clusters.
Nevertheless, since the inner for loop and assignment is independent of every other iteration,
it is an example of ``embarrassingly'' parallel programming, and as such using a parallel for
loop construct is enough to evenly distribute the work between $n$ number of processors.

For the third step, the update step, a parallel reduce was used to sum up the values of the particles
before perform a division to get the mean. While it is also possible to use a parallel for construct
over the outer for loop, it does not scale well if $k$ is smaller than the number of processors.
A parallel reduce would almost always utilize the processors fully since the number particles usually
outnumber the number processors.

\subsection{k-means++}
While the original algorithms runs fine by randomly choosing \textit{k}-clusters
as the initial clusters, it has been shown that the worst case running time
can be super-polynomial. By using the k-means++ algorithm to choose initial
starting clusters, the algorithm is guaranteed a solution that is O(log k)
competitive to the optimal k-means solution.

The fundamental idea of the k-means++ algorithm is to prevent clusters from being to close together
and spread out where the clusters are.

\begin{algorithm}
\caption{k-means++ algorithm}
\begin{algorithmic}[1]
\STATE{choose one center at random among the data points}
\FOR{each data point $x$}
    \STATE{compute D(x), the distance between $x$ and the nearest center already chosen}
\ENDFOR
\STATE{add one data point as a cluster, using a weighted probability distribution where a point $x$ is chosen with probability proportional to D$(x)^2$}
\STATE{repeat steps 2 to 5 until $k$ centers have been chosen}
\STATE{run standard k-means clustering algorithm}
\end{algorithmic}
\end{algorithm}

Similar to the regular k-means algorithm, the time dominant step for k-means++ is the first step
since for every particle, one must also iterate over every cluster. Since the computation of
finding which cluster has the minimum distance to a particle is independent of one another,
a parallel for loop would suffice in efficiently parallelizing the section of the code.

The second part of the algorithm, which involves a prefix sum over an
array ofshortest distances to a known center, can be implemented as a parallel prefix scan.

\section{Implementation}

In order to implement parallelization in the k-means program, we chose the
Intel's Threading Building Block (TBB) framework since all the necessary parallel primitives
desired are present. By utilizing TBB, the programmer does not become concerned with
low-level thread management since it is automatically managed by a scheduler.
The only variable in the TBB that we which to tune is the grain size for the parallel\_for
construct, since this a determinant of total run time.

Because of the decision to use TBB, C++ had to be used by default as a programming language since
the TBB primitives expects to interface with classes as primitives. Although overhead should be minimal,
C++ class constructors and destructors might contribute slightly to runtime.

In addition to using TBB, we also decided to implement k-means using OpenMP as a baseline for comparison.
Determining whether TBB possessed some form of overhead was an important goal, and since OpenMP is
implemented in every major compiler, it would also be interesting what sort of performance one could
get without using a specialized libarary.

In order to determine the effectiveness of parallelization, several experiments regarding
grain size, paricle size, core size, and cluster size were run. Determining the optimal grain size
is important since an improper grain size might create unnecessary (if too small) or not be able
to fully utilize the total number of cores (if too large). Particle size is also important since
this has implications of what resolutions today's technology can process at a reasonable speed.
If a certain response time is desired of a machine vision program, it might become necessary
to down-sample images received. Another aspect of interests regarding this algorithm is the
number of processors. At what point would one start seeing the benefits of parallelization?
Would one expect a linear speedup to an infinitie number of processors? Lastly, we would like
to answer what an efficient cluster size would be. If an image has too many objects that need
to be segmented, would the algorithm become unusable? Also, is the inverse true?

\section{Results}

In this section, all graphs all normalized to the fastest serial version, unless mentioned otherwise.
The fastest serial version was compiled GCC version 4.3.2 with maximum optimization (-O3) and loops
unrolled (-funroll-all-loops). Additionally, cluster size is held constant at $k=50$ for all experiments
except the last one which determines optimal cluster size.

All runtime experiments were performed on the SEAS acgrid30 - acgrid32 clusters, unless mentioned otherwise.
The machines we chose have 4 distinct cores that can run at least 4 hardware threads at a time. The image that we chose was
of the town of Porto Moniz with a resolution of $1280 \times 800$ pixels. This equates to
1,024,000 pixels.

\begin{figure}[htp]
\centering
\includegraphics[width=.90\linewidth,bb=0 0 1280 800]{portomoniz.jpg}
\caption{The town of Porto Moniz}
\end{figure}

\subsection{Speedup vs. Grain Size}

\includegraphics[width=.90\linewidth]{graph_grainsize}

As expected, TBB produce a somewhat ideal speedup until grain size got too large for to be evenly
distributed evenly over the number of cores. Surprisingly, OpenMP exhibits super-linear speedup
with the static and dynamic scheduler. Either the grainsize configuration didn't work, or
static values were prechosen, since varying grain size did not significantly affect grainsize.

\subsection{Speedup vs. Number of Particles}

In this experiment, we scaled the above image to vary the number of particles.

\includegraphics[width=.90\linewidth]{graph_speedup_vs_particle_size}

When the speedup is compared to the particle size, a particle size that is
too small results in a sub-optimal speedup. However, as the particle size
gets larger to more reasonable values, the speedup remains fairly linear for
TBB and and a decrease to the maximum theoretical linear speedup for OpenMP.

\includegraphics[width=.90\linewidth]{graph_best_time_vs_particle_size}

Since all the previous graphs were in terms of normalized speedups,
this graph was included to give a sense of runtimes on four cores.
This graph shows that run-time linearly scales with particle size with
OpenMP producing better results.

\subsection{Speedup vs. Number of Cores}

\includegraphics[width=.90\linewidth]{graph_cores}

Experimentation shows that TBB scales fairly linearly with the number of cores,
achieving a near linear speedup. However, OpenMP levels out at around 4.0, and shows
no further improvement.

\subsection{Speedup vs. Number of Clusters}

\includegraphics[width=.90\linewidth]{graph_clusters}

For TBB, speedup is fairly linear on four cores when compared to the serial version, except
for when values of $k$ are extremely small. Like the graphs, before OpenMP exhibits better
peformance than TBB in almost all cases.

\section{Discussion of Results}

\subsection{Speedup vs. Grain Size}
In the case of OpenMP, the speedup is irrelevant of grain size until it becomes so large
that total particles divided by grain size is larger than the number of cores present. On
the hand, with OpenMP, superlinear speedup is achieved, which is surprising.
In order to explain this, several theories were created. Initially it is thought that
loop unrolling might be the cause; however, the linear version also had the loop unrolling
compilation flag, so this possibility is eliminated. Upon brief inspection of the assembly
code in the GNU debugger (gdb), it was revealed that gcc made significant use of the
Intel Single Instruction Multiple Data (SIMD) in a fashion that is better than TBB.
Unfortunately, there was not enough time to study the disassembly of the libraries in depth.
Lastly, another hypothesis was that OpenMP better utlized the memory bus than TBB, either
producing a sequence of instructions that read with more throughput or produce a better
read ahead effect. Cache size is believed to be so strongly involved since our image
produces a \texttt{double} array of size $1024000 \textrm{ particles}
\times 8 \textrm{ bytes} \times 3 \textrm{ channels} = 23.4375$ MB, which should definitely
be larger than the L2 cache.

\subsection{Speedup vs. Number of Particles}

Intel's TBB exhibit a stable almost unvarying level speedup after particle size is large
enough to ensure each of the cores have enough work distributed to them. The same is true
for OpenMP, where if the work is too small, speedup gains are not as efficient as they
can be. This seemingly reflects one of the basic rules of thumb in parallel programming:
if your data set is too small, it is better to fallback to the serial version instead of
creating the extra overhead of parallelization and gaining no benefit from it. Like
the above results for speedup vs. grain size, the OpenMP seems to get astonishing
superlinear speedups. Since this part of the experiment involves manipulating large sets
of data, the theory of better memory throughput or readahead effect seems to be supported
since read-ahead becomes much more apparent when the number of particles is small (resulting
in all of the particles to be read into the L2 cache). As particle size increases, the affects
of read-ahead seems to level off, but memory throughput is still higher than TBB. The
slope in performance with increasing particle size seems still disqualify any theory that
involves OpenMP code being able to perform more FLOPs per second than TBB code as the explanation
for the increased performance. Such a line would be much more linear since instructions would have
consume a near constant number of FLOPs per particle.

\subsection{Speedup vs. Number of Cores}

\subsection{Speedup vs. Number of Clusters}

\section{Conclusion}

% needed in second column of first page if using \pubid
%\pubidadjcol

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}
% references section
%\bibliographystyle{IEEEtran.bst}
%\bibliography{IEEEabrv,../bib/paper}
\begin{thebibliography}{1}

\bibitem{IEEEhowto:kopka}
H.~Kopka and P.~W. Daly, \emph{A Guide to {\LaTeX}}, 3rd~ed.
Harlow, England: Addison-Wesley, 1999.

\end{thebibliography}

% biography section
% 
%\begin{biographynophoto}{Chris Jordan}
%Biography text here.
%\end{biographynophoto}

% if you will not have a photo
%\begin{biographynophoto}{Victor Janmey}
%Biography text here.
%\end{biographynophoto}

% insert where needed to balance the two columns on the last page
%\newpage

%\begin{biographynophoto}{Raymond Ko}
%Biography text here.
%\end{biographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}

\end{document}
